{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/mohamedelhakim/Transformer-CNN-Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hybrid model combines a Convolutional Neural Network (CNN) and a Transformer to classify ECG signals. The CNN first extracts local features from the input signal, capturing patterns like peaks and troughs. The output of the CNN is then passed to a Transformer, which processes these features to capture long-range dependencies, learning relationships between distant points in the signal. This approach leverages the strengths of both architectures: the CNN’s ability to focus on local patterns and the Transformer’s capacity to understand global context. The final output is a classification of the ECG signal, benefiting from both local and global feature learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics.classification import Accuracy\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X=rnnamp_model['text']\n",
    "# y=np.array(rnnamp_model['labels'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnnamp_model = pd.read_csv('/kaggle/input/hemolytic/combined.csv')\n",
    "\n",
    "dataframes = {}\n",
    "directory_path = 'Heartbeat_Dataset'\n",
    "all_files = os.listdir(directory_path)\n",
    "\n",
    "for file in os.listdir(directory_path):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(directory_path, file)\n",
    "        # Remove the .csv extension for the DataFrame name\n",
    "        df_name = os.path.splitext(file)[0]\n",
    "        dataframes[df_name] = pd.read_csv(file_path, header= None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = dataframes['mitbih_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n",
      "N    72471\n",
      "Q     6431\n",
      "V     5788\n",
      "S     2223\n",
      "F      641\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\randa\\AppData\\Local\\Temp\\ipykernel_10088\\4019137323.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0        N\n",
      "1        N\n",
      "2        N\n",
      "3        N\n",
      "4        N\n",
      "        ..\n",
      "87549    Q\n",
      "87550    Q\n",
      "87551    Q\n",
      "87552    Q\n",
      "87553    Q\n",
      "Name: 187, Length: 87554, dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  train_df.iloc[:, -1] = train_df.iloc[:, -1].replace(labels)\n"
     ]
    }
   ],
   "source": [
    "labels = {\n",
    "    0.0: \"N\",\n",
    "    1.0: \"S\",\n",
    "    2.0: \"V\",\n",
    "    3.0: \"F\",\n",
    "    4.0: \"Q\"\n",
    "}\n",
    "\n",
    "train_df.iloc[:, -1] = train_df.iloc[:, -1].replace(labels)\n",
    "\n",
    "# Now get the value counts for the renamed last column\n",
    "train_counts = train_df.iloc[:, -1].value_counts()\n",
    "\n",
    "# Print the counts\n",
    "print(train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.977941</td>\n",
       "      <td>0.926471</td>\n",
       "      <td>0.681373</td>\n",
       "      <td>0.245098</td>\n",
       "      <td>0.154412</td>\n",
       "      <td>0.191176</td>\n",
       "      <td>0.151961</td>\n",
       "      <td>0.085784</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.049020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.960114</td>\n",
       "      <td>0.863248</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.196581</td>\n",
       "      <td>0.094017</td>\n",
       "      <td>0.125356</td>\n",
       "      <td>0.099715</td>\n",
       "      <td>0.088319</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.082621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.659459</td>\n",
       "      <td>0.186486</td>\n",
       "      <td>0.070270</td>\n",
       "      <td>0.070270</td>\n",
       "      <td>0.059459</td>\n",
       "      <td>0.056757</td>\n",
       "      <td>0.043243</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.045946</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.925414</td>\n",
       "      <td>0.665746</td>\n",
       "      <td>0.541436</td>\n",
       "      <td>0.276243</td>\n",
       "      <td>0.196133</td>\n",
       "      <td>0.077348</td>\n",
       "      <td>0.071823</td>\n",
       "      <td>0.060773</td>\n",
       "      <td>0.066298</td>\n",
       "      <td>0.058011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.967136</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.830986</td>\n",
       "      <td>0.586854</td>\n",
       "      <td>0.356808</td>\n",
       "      <td>0.248826</td>\n",
       "      <td>0.145540</td>\n",
       "      <td>0.089202</td>\n",
       "      <td>0.117371</td>\n",
       "      <td>0.150235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87549</th>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.494737</td>\n",
       "      <td>0.536842</td>\n",
       "      <td>0.529825</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.484211</td>\n",
       "      <td>0.456140</td>\n",
       "      <td>0.396491</td>\n",
       "      <td>0.284211</td>\n",
       "      <td>0.136842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87550</th>\n",
       "      <td>0.718333</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>0.486667</td>\n",
       "      <td>0.361667</td>\n",
       "      <td>0.231667</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.051667</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87551</th>\n",
       "      <td>0.906122</td>\n",
       "      <td>0.624490</td>\n",
       "      <td>0.595918</td>\n",
       "      <td>0.575510</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.481633</td>\n",
       "      <td>0.444898</td>\n",
       "      <td>0.387755</td>\n",
       "      <td>0.322449</td>\n",
       "      <td>0.191837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87552</th>\n",
       "      <td>0.858228</td>\n",
       "      <td>0.645570</td>\n",
       "      <td>0.845570</td>\n",
       "      <td>0.248101</td>\n",
       "      <td>0.167089</td>\n",
       "      <td>0.131646</td>\n",
       "      <td>0.121519</td>\n",
       "      <td>0.121519</td>\n",
       "      <td>0.118987</td>\n",
       "      <td>0.103797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87553</th>\n",
       "      <td>0.901506</td>\n",
       "      <td>0.845886</td>\n",
       "      <td>0.800695</td>\n",
       "      <td>0.748552</td>\n",
       "      <td>0.687138</td>\n",
       "      <td>0.599073</td>\n",
       "      <td>0.512167</td>\n",
       "      <td>0.427578</td>\n",
       "      <td>0.395133</td>\n",
       "      <td>0.402086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87554 rows × 188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0      0.977941  0.926471  0.681373  0.245098  0.154412  0.191176  0.151961   \n",
       "1      0.960114  0.863248  0.461538  0.196581  0.094017  0.125356  0.099715   \n",
       "2      1.000000  0.659459  0.186486  0.070270  0.070270  0.059459  0.056757   \n",
       "3      0.925414  0.665746  0.541436  0.276243  0.196133  0.077348  0.071823   \n",
       "4      0.967136  1.000000  0.830986  0.586854  0.356808  0.248826  0.145540   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "87549  0.807018  0.494737  0.536842  0.529825  0.491228  0.484211  0.456140   \n",
       "87550  0.718333  0.605000  0.486667  0.361667  0.231667  0.120000  0.051667   \n",
       "87551  0.906122  0.624490  0.595918  0.575510  0.530612  0.481633  0.444898   \n",
       "87552  0.858228  0.645570  0.845570  0.248101  0.167089  0.131646  0.121519   \n",
       "87553  0.901506  0.845886  0.800695  0.748552  0.687138  0.599073  0.512167   \n",
       "\n",
       "            7         8         9    ...  178  179  180  181  182  183  184  \\\n",
       "0      0.085784  0.058824  0.049020  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1      0.088319  0.074074  0.082621  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2      0.043243  0.054054  0.045946  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3      0.060773  0.066298  0.058011  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4      0.089202  0.117371  0.150235  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "...         ...       ...       ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "87549  0.396491  0.284211  0.136842  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "87550  0.001667  0.000000  0.013333  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "87551  0.387755  0.322449  0.191837  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "87552  0.121519  0.118987  0.103797  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "87553  0.427578  0.395133  0.402086  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "       185  186  187  \n",
       "0      0.0  0.0    N  \n",
       "1      0.0  0.0    N  \n",
       "2      0.0  0.0    N  \n",
       "3      0.0  0.0    N  \n",
       "4      0.0  0.0    N  \n",
       "...    ...  ...  ...  \n",
       "87549  0.0  0.0    Q  \n",
       "87550  0.0  0.0    Q  \n",
       "87551  0.0  0.0    Q  \n",
       "87552  0.0  0.0    Q  \n",
       "87553  0.0  0.0    Q  \n",
       "\n",
       "[87554 rows x 188 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = train_df.iloc[:,:187]\n",
    "y_label = train_df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187\n",
       "N    72471\n",
       "Q     6431\n",
       "V     5788\n",
       "S     2223\n",
       "F      641\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y_label)  # Encode labels without replacement\n",
    "num_classes = len(label_encoder.classes_)  # Use the number of unique classes\n",
    "y = np.eye(num_classes)[y] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.expand_dims(X_train, axis=1)  # Change axis from 2 to 1\n",
    "X_test = np.expand_dims(X_test, axis=1)    # Change axis from 2 to 1\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train).float()  # Ensure the data is of float type\n",
    "X_test_tensor = torch.tensor(X_test).float()    # Ensure the data is of float type\n",
    "y_train_tensor = torch.tensor(y_train).long()   # Ensure labels are long type\n",
    "y_test_tensor = torch.tensor(y_test).long()   \n",
    "\n",
    "y_train_tensor = y_train_tensor.argmax(dim=1)  # Reduces [70043, 5] to [70043]\n",
    "y_test_tensor = y_test_tensor.argmax(dim=1)\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_tensor shape: torch.Size([70043, 1, 187])\n",
      "y_train_tensor shape: torch.Size([70043])\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train_tensor shape:\", X_train_tensor.shape)  #\n",
    "print(\"y_train_tensor shape:\", y_train_tensor.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.TensorDataset object at 0x0000020BFABC5E90>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x0000020BFC703390>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNTransformerHybrid(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, num_heads, num_layers, d_model=128):\n",
    "        super(CNNTransformerHybrid, self).__init__()\n",
    "\n",
    "        # CNN Feature extractor with Conv1d\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        \n",
    "        # MaxPooling layer\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        # Batch normalization after convolutions\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "\n",
    "        # Projection layer to match transformer input size\n",
    "        self.projector = nn.Linear(256, d_model)\n",
    "\n",
    "        # Transformer encoder\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads), num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # Fully connected layer to output the final classification\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure input is of shape (batch_size, channels, seq_len)\n",
    "        if x.ndimension() == 2:\n",
    "            x = x.unsqueeze(1)  # Add channel dimension\n",
    "        \n",
    "        # Convolutional layers with ReLU, batch normalization, and pooling\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x)))) \n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x)))) \n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x)))) \n",
    "        \n",
    "        # Flattening and projection for transformer input\n",
    "        x = x.permute(0, 2, 1)  # (batch_size, seq_len, channels)\n",
    "        x = self.projector(x)  # Project to transformer input size\n",
    "\n",
    "        # Transformer encoding\n",
    "        x = x.permute(1, 0, 2)  # Change to (seq_len, batch_size, d_model)\n",
    "        x = self.encoder(x)  # Apply transformer layers\n",
    "\n",
    "        # Pool the transformer output (use the last token for classification or apply mean)\n",
    "        x = x.mean(dim=0)  # Aggregate over sequence length (or use last token: x[-1])\n",
    "        x = self.fc(self.dropout(x))  # Fully connected layer after dropout\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Model instantiation\n",
    "model = CNNTransformerHybrid(\n",
    "    input_dim=187,  # Number of features per sample (input dimension)\n",
    "    num_classes=5,  # Number of output classes\n",
    "    num_heads=8,    # Multi-head attention heads\n",
    "    num_layers=6,   # Number of Transformer layers\n",
    "    d_model=128     # Dimension of transformer model\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNTransformerHybrid(\n",
      "  (conv1): Conv1d(1, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv2): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv3): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (projector): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.2267, Train Accuracy: 0.9409\n",
      "Test Loss: 0.1271, Test Accuracy: 0.9672\n",
      "Epoch 2/10\n",
      "Train Loss: 0.1436, Train Accuracy: 0.9638\n",
      "Test Loss: 0.1046, Test Accuracy: 0.9724\n",
      "Epoch 3/10\n",
      "Train Loss: 0.1176, Train Accuracy: 0.9699\n",
      "Test Loss: 0.0979, Test Accuracy: 0.9733\n",
      "Epoch 4/10\n",
      "Train Loss: 0.1038, Train Accuracy: 0.9736\n",
      "Test Loss: 0.0876, Test Accuracy: 0.9781\n",
      "Epoch 5/10\n",
      "Train Loss: 0.0903, Train Accuracy: 0.9767\n",
      "Test Loss: 0.0767, Test Accuracy: 0.9796\n",
      "Epoch 6/10\n",
      "Train Loss: 0.0823, Train Accuracy: 0.9787\n",
      "Test Loss: 0.0826, Test Accuracy: 0.9785\n",
      "Epoch 7/10\n",
      "Train Loss: 0.0772, Train Accuracy: 0.9806\n",
      "Test Loss: 0.0790, Test Accuracy: 0.9783\n",
      "Epoch 8/10\n",
      "Train Loss: 0.0710, Train Accuracy: 0.9811\n",
      "Test Loss: 0.0747, Test Accuracy: 0.9813\n",
      "Epoch 9/10\n",
      "Train Loss: 0.0653, Train Accuracy: 0.9825\n",
      "Test Loss: 0.0647, Test Accuracy: 0.9842\n",
      "Epoch 10/10\n",
      "Train Loss: 0.0619, Train Accuracy: 0.9834\n",
      "Test Loss: 0.0638, Test Accuracy: 0.9828\n"
     ]
    }
   ],
   "source": [
    "# Define a simple training and testing loop\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # No need to compute gradients during testing\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Get the predicted class\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            \n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            \n",
    "            # Save predictions and labels for additional evaluation if needed\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(target.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(test_loader)\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels\n",
    "\n",
    "\n",
    "# Set the device (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model\n",
    "model = CNNTransformerHybrid(\n",
    "    input_dim=187,  # Number of features per sample (input dimension)\n",
    "    num_classes=5,  # Number of output classes\n",
    "    num_heads=8,  # Multi-head attention heads\n",
    "    num_layers=6  # Number of Transformer layers\n",
    ").to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)  # Optimizer with a learning rate of 1e-4\n",
    "\n",
    "# Number of epochs to train\n",
    "epochs = 10\n",
    "train_accuracies  = []  \n",
    "test_accuracies  = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train the model\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "    train_accuracies.append(train_acc) \n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "    \n",
    "    # Test the model\n",
    "    test_loss, test_acc, preds, labels = test(model, test_loader, criterion, device)\n",
    "    test_accuracies.append(test_acc)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Optionally: Save the trained model\n",
    "# torch.save(model.state_dict(), \"cnn_transformer_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_epochs = min(len(train_acc), len(test_acc))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(num_epochs), train_acc[:num_epochs], label='Training Accuracy')\n",
    "plt.plot(np.arange(num_epochs), test_acc[:num_epochs], label='Test Accuracy')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Test Accuracy Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./model/cnn_transformer_model.pth\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./model/cnn_transformer_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21892, 188)\n"
     ]
    }
   ],
   "source": [
    "test_df = dataframes['mitbih_test']\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    0.0: \"N\",\n",
    "    1.0: \"S\",\n",
    "    2.0: \"V\",\n",
    "    3.0: \"F\",\n",
    "    4.0: \"Q\"\n",
    "}\n",
    "\n",
    "train_df.iloc[:, -1] = train_df.iloc[:, -1].replace(labels)\n",
    "\n",
    "x_data = train_df.iloc[:,:187]\n",
    "y_label = train_df.iloc[:,-1]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y_label.values.ravel())\n",
    "\n",
    "\n",
    "# Prepare data for model\n",
    "X = x_data\n",
    "X_test = np.expand_dims(X, axis=1)    # Add a new dimension to match model input\n",
    "X_test_tensor = torch.tensor(X_test).float()    # Convert to float tensor\n",
    "y_test_tensor = torch.tensor(y).long()          # Convert to long tensor for labels\n",
    "\n",
    "# Create DataLoader\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Evaluate the model\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # No need to compute gradients during testing\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Get the predicted class\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            \n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            \n",
    "            # Save predictions and labels for additional evaluation if needed\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(target.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(test_loader)\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0501, Test Accuracy: 0.9862\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()  # For multi-class classification\n",
    "\n",
    "epoch_loss, epoch_acc, all_preds, all_labels = test(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {epoch_loss:.4f}, Test Accuracy: {epoch_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
