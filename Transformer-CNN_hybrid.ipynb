{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/mohamedelhakim/Transformer-CNN-Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X=rnnamp_model['text']\n",
    "# y=np.array(rnnamp_model['labels'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnnamp_model = pd.read_csv('/kaggle/input/hemolytic/combined.csv')\n",
    "\n",
    "dataframes = {}\n",
    "directory_path = 'Heartbeat_Dataset'\n",
    "all_files = os.listdir(directory_path)\n",
    "\n",
    "for file in os.listdir(directory_path):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(directory_path, file)\n",
    "        # Remove the .csv extension for the DataFrame name\n",
    "        df_name = os.path.splitext(file)[0]\n",
    "        dataframes[df_name] = pd.read_csv(file_path, header= None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = dataframes['mitbih_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n",
      "N    72471\n",
      "Q     6431\n",
      "V     5788\n",
      "S     2223\n",
      "F      641\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\randa\\AppData\\Local\\Temp\\ipykernel_10380\\4019137323.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0        N\n",
      "1        N\n",
      "2        N\n",
      "3        N\n",
      "4        N\n",
      "        ..\n",
      "87549    Q\n",
      "87550    Q\n",
      "87551    Q\n",
      "87552    Q\n",
      "87553    Q\n",
      "Name: 187, Length: 87554, dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  train_df.iloc[:, -1] = train_df.iloc[:, -1].replace(labels)\n"
     ]
    }
   ],
   "source": [
    "labels = {\n",
    "    0.0: \"N\",\n",
    "    1.0: \"S\",\n",
    "    2.0: \"V\",\n",
    "    3.0: \"F\",\n",
    "    4.0: \"Q\"\n",
    "}\n",
    "\n",
    "train_df.iloc[:, -1] = train_df.iloc[:, -1].replace(labels)\n",
    "\n",
    "# Now get the value counts for the renamed last column\n",
    "train_counts = train_df.iloc[:, -1].value_counts()\n",
    "\n",
    "# Print the counts\n",
    "print(train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.977941</td>\n",
       "      <td>0.926471</td>\n",
       "      <td>0.681373</td>\n",
       "      <td>0.245098</td>\n",
       "      <td>0.154412</td>\n",
       "      <td>0.191176</td>\n",
       "      <td>0.151961</td>\n",
       "      <td>0.085784</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.049020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.960114</td>\n",
       "      <td>0.863248</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.196581</td>\n",
       "      <td>0.094017</td>\n",
       "      <td>0.125356</td>\n",
       "      <td>0.099715</td>\n",
       "      <td>0.088319</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.082621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.659459</td>\n",
       "      <td>0.186486</td>\n",
       "      <td>0.070270</td>\n",
       "      <td>0.070270</td>\n",
       "      <td>0.059459</td>\n",
       "      <td>0.056757</td>\n",
       "      <td>0.043243</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.045946</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.925414</td>\n",
       "      <td>0.665746</td>\n",
       "      <td>0.541436</td>\n",
       "      <td>0.276243</td>\n",
       "      <td>0.196133</td>\n",
       "      <td>0.077348</td>\n",
       "      <td>0.071823</td>\n",
       "      <td>0.060773</td>\n",
       "      <td>0.066298</td>\n",
       "      <td>0.058011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.967136</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.830986</td>\n",
       "      <td>0.586854</td>\n",
       "      <td>0.356808</td>\n",
       "      <td>0.248826</td>\n",
       "      <td>0.145540</td>\n",
       "      <td>0.089202</td>\n",
       "      <td>0.117371</td>\n",
       "      <td>0.150235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87549</th>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.494737</td>\n",
       "      <td>0.536842</td>\n",
       "      <td>0.529825</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.484211</td>\n",
       "      <td>0.456140</td>\n",
       "      <td>0.396491</td>\n",
       "      <td>0.284211</td>\n",
       "      <td>0.136842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87550</th>\n",
       "      <td>0.718333</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>0.486667</td>\n",
       "      <td>0.361667</td>\n",
       "      <td>0.231667</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.051667</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87551</th>\n",
       "      <td>0.906122</td>\n",
       "      <td>0.624490</td>\n",
       "      <td>0.595918</td>\n",
       "      <td>0.575510</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.481633</td>\n",
       "      <td>0.444898</td>\n",
       "      <td>0.387755</td>\n",
       "      <td>0.322449</td>\n",
       "      <td>0.191837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87552</th>\n",
       "      <td>0.858228</td>\n",
       "      <td>0.645570</td>\n",
       "      <td>0.845570</td>\n",
       "      <td>0.248101</td>\n",
       "      <td>0.167089</td>\n",
       "      <td>0.131646</td>\n",
       "      <td>0.121519</td>\n",
       "      <td>0.121519</td>\n",
       "      <td>0.118987</td>\n",
       "      <td>0.103797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87553</th>\n",
       "      <td>0.901506</td>\n",
       "      <td>0.845886</td>\n",
       "      <td>0.800695</td>\n",
       "      <td>0.748552</td>\n",
       "      <td>0.687138</td>\n",
       "      <td>0.599073</td>\n",
       "      <td>0.512167</td>\n",
       "      <td>0.427578</td>\n",
       "      <td>0.395133</td>\n",
       "      <td>0.402086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87554 rows × 188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0      0.977941  0.926471  0.681373  0.245098  0.154412  0.191176  0.151961   \n",
       "1      0.960114  0.863248  0.461538  0.196581  0.094017  0.125356  0.099715   \n",
       "2      1.000000  0.659459  0.186486  0.070270  0.070270  0.059459  0.056757   \n",
       "3      0.925414  0.665746  0.541436  0.276243  0.196133  0.077348  0.071823   \n",
       "4      0.967136  1.000000  0.830986  0.586854  0.356808  0.248826  0.145540   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "87549  0.807018  0.494737  0.536842  0.529825  0.491228  0.484211  0.456140   \n",
       "87550  0.718333  0.605000  0.486667  0.361667  0.231667  0.120000  0.051667   \n",
       "87551  0.906122  0.624490  0.595918  0.575510  0.530612  0.481633  0.444898   \n",
       "87552  0.858228  0.645570  0.845570  0.248101  0.167089  0.131646  0.121519   \n",
       "87553  0.901506  0.845886  0.800695  0.748552  0.687138  0.599073  0.512167   \n",
       "\n",
       "            7         8         9    ...  178  179  180  181  182  183  184  \\\n",
       "0      0.085784  0.058824  0.049020  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1      0.088319  0.074074  0.082621  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2      0.043243  0.054054  0.045946  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3      0.060773  0.066298  0.058011  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4      0.089202  0.117371  0.150235  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "...         ...       ...       ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "87549  0.396491  0.284211  0.136842  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "87550  0.001667  0.000000  0.013333  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "87551  0.387755  0.322449  0.191837  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "87552  0.121519  0.118987  0.103797  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "87553  0.427578  0.395133  0.402086  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "       185  186  187  \n",
       "0      0.0  0.0    N  \n",
       "1      0.0  0.0    N  \n",
       "2      0.0  0.0    N  \n",
       "3      0.0  0.0    N  \n",
       "4      0.0  0.0    N  \n",
       "...    ...  ...  ...  \n",
       "87549  0.0  0.0    Q  \n",
       "87550  0.0  0.0    Q  \n",
       "87551  0.0  0.0    Q  \n",
       "87552  0.0  0.0    Q  \n",
       "87553  0.0  0.0    Q  \n",
       "\n",
       "[87554 rows x 188 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = train_df.iloc[:,:187]\n",
    "y_label = train_df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187\n",
       "N    72471\n",
       "Q     6431\n",
       "V     5788\n",
       "S     2223\n",
       "F      641\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y_label)  # Encode labels without replacement\n",
    "num_classes = len(label_encoder.classes_)  # Use the number of unique classes\n",
    "y = np.eye(num_classes)[y] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.expand_dims(X_train, axis=1)  # Change axis from 2 to 1\n",
    "X_test = np.expand_dims(X_test, axis=1)    # Change axis from 2 to 1\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train).float()  # Ensure the data is of float type\n",
    "X_test_tensor = torch.tensor(X_test).float()    # Ensure the data is of float type\n",
    "y_train_tensor = torch.tensor(y_train).long()   # Ensure labels are long type\n",
    "y_test_tensor = torch.tensor(y_test).long()   \n",
    "\n",
    "y_train_tensor = y_train_tensor.argmax(dim=1)  # Reduces [70043, 5] to [70043]\n",
    "y_test_tensor = y_test_tensor.argmax(dim=1)\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_tensor shape: torch.Size([70043, 1, 187])\n",
      "y_train_tensor shape: torch.Size([70043])\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train_tensor shape:\", X_train_tensor.shape)  #\n",
    "print(\"y_train_tensor shape:\", y_train_tensor.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.TensorDataset object at 0x00000182A9AB55D0>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000182AFB8EC10>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\randa\\OneDrive\\Documents\\GitHub\\Neural-Networks-Project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3)\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3)\n",
    "        \n",
    "        # Max pooling\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Flatten layer\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256 * self._get_conv_output_size(187), 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def _get_conv_output_size(self, input_length):\n",
    "        output_length = input_length\n",
    "        # Calculate the output length after each convolution and pooling operation\n",
    "        for _ in range(3):  # Three convolution layers\n",
    "            output_length = (output_length - 2) // 2  # Pooling reduces the length by half after each convolution\n",
    "        return output_length\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers followed by ReLU and pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # First convolution + pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # Second convolution + pooling\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # Third convolution + pooling\n",
    "        \n",
    "        # Flatten the tensor output before passing it through the fully connected layers\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # Fully connected layer + dropout\n",
    "        x = F.relu(self.fc1(x))  # First fully connected layer\n",
    "        x = self.dropout(x)      # Dropout layer\n",
    "        x = self.fc2(x)          # Final output layer\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "# CNN-Transformer Hybrid Model\n",
    "class CNNTransformerHybrid(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, num_heads, num_layers):\n",
    "        super(CNNTransformerHybrid, self).__init__()\n",
    "\n",
    "        # CNN Feature extractor with Conv1d\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        \n",
    "        # Transformer Encoder setup\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=128, nhead=num_heads), num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer to output the final classification\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 2:\n",
    "            x = x.unsqueeze(1)  # Add channel dimension: (batch_size, 1, seq_len)\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # First Conv1d layer + Pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # Second Conv1d layer + Pooling\n",
    "        \n",
    "        # Step 2: Transformer input preparation\n",
    "        x = x.permute(2, 0, 1)  # Change shape to (seq_len, batch_size, features) for the transformer\n",
    "        x = self.encoder(x)  # Transformer Encoder\n",
    "        \n",
    "        # Step 3: Classification\n",
    "        x = x.mean(dim=0)  # Aggregate over sequence length\n",
    "        x = self.fc(x)  # Fully connected layer to get final class prediction\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Instantiation\n",
    "model = CNNTransformerHybrid(\n",
    "    input_dim=187,  # Number of features per sample (input dimension)\n",
    "    num_classes=10,  # Number of output classes\n",
    "    num_heads=8,  # Multi-head attention heads\n",
    "    num_layers=6  # Number of Transformer layers\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNTransformerHybrid(\n",
      "  (conv1): Conv1d(1, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv2): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.3072, Train Accuracy: 0.9156\n",
      "Test Loss: 0.1972, Test Accuracy: 0.9475\n",
      "Epoch 2/10\n",
      "Train Loss: 0.1844, Train Accuracy: 0.9495\n",
      "Test Loss: 0.1562, Test Accuracy: 0.9572\n",
      "Epoch 3/10\n",
      "Train Loss: 0.1511, Train Accuracy: 0.9573\n",
      "Test Loss: 0.1246, Test Accuracy: 0.9649\n",
      "Epoch 4/10\n",
      "Train Loss: 0.1262, Train Accuracy: 0.9651\n",
      "Test Loss: 0.1078, Test Accuracy: 0.9702\n",
      "Epoch 5/10\n",
      "Train Loss: 0.1115, Train Accuracy: 0.9685\n",
      "Test Loss: 0.1158, Test Accuracy: 0.9678\n",
      "Epoch 6/10\n",
      "Train Loss: 0.1007, Train Accuracy: 0.9714\n",
      "Test Loss: 0.1364, Test Accuracy: 0.9653\n",
      "Epoch 7/10\n",
      "Train Loss: 0.0933, Train Accuracy: 0.9740\n",
      "Test Loss: 0.0961, Test Accuracy: 0.9739\n",
      "Epoch 8/10\n",
      "Train Loss: 0.0887, Train Accuracy: 0.9748\n",
      "Test Loss: 0.0843, Test Accuracy: 0.9767\n",
      "Epoch 9/10\n",
      "Train Loss: 0.0837, Train Accuracy: 0.9761\n",
      "Test Loss: 0.0870, Test Accuracy: 0.9758\n",
      "Epoch 10/10\n",
      "Train Loss: 0.0806, Train Accuracy: 0.9769\n",
      "Test Loss: 0.0809, Test Accuracy: 0.9771\n"
     ]
    }
   ],
   "source": [
    "# Define a simple training and testing loop\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimize the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update the running loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Get the predicted class\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        \n",
    "        # Update the total and correct count\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # No need to compute gradients during testing\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Get the predicted class\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            \n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            \n",
    "            # Save predictions and labels for additional evaluation if needed\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(target.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(test_loader)\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels\n",
    "\n",
    "\n",
    "# Set the device (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model\n",
    "model = CNNTransformerHybrid(\n",
    "    input_dim=187,  # Number of features per sample (input dimension)\n",
    "    num_classes=10,  # Number of output classes\n",
    "    num_heads=8,  # Multi-head attention heads\n",
    "    num_layers=6  # Number of Transformer layers\n",
    ").to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)  # Optimizer with a learning rate of 1e-4\n",
    "\n",
    "# Number of epochs to train\n",
    "epochs = 10\n",
    "\n",
    "# Training and testing loops\n",
    "for epoch in range(epochs):\n",
    "    # Train the model\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "    \n",
    "    # Test the model\n",
    "    test_loss, test_acc, preds, labels = test(model, test_loader, criterion, device)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Optionally: Save the trained model\n",
    "# torch.save(model.state_dict(), \"cnn_transformer_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./model/cnn_transformer_model.pth\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./model/cnn_transformer_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21892, 188)\n"
     ]
    }
   ],
   "source": [
    "test_df = dataframes['mitbih_test']\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    0.0: \"N\",\n",
    "    1.0: \"S\",\n",
    "    2.0: \"V\",\n",
    "    3.0: \"F\",\n",
    "    4.0: \"Q\"\n",
    "}\n",
    "\n",
    "train_df.iloc[:, -1] = train_df.iloc[:, -1].replace(labels)\n",
    "\n",
    "x_data = train_df.iloc[:,:187]\n",
    "y_label = train_df.iloc[:,-1]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y_label.values.ravel())\n",
    "\n",
    "\n",
    "# Prepare data for model\n",
    "X = x_data\n",
    "X_test = np.expand_dims(X, axis=1)    # Add a new dimension to match model input\n",
    "X_test_tensor = torch.tensor(X_test).float()    # Convert to float tensor\n",
    "y_test_tensor = torch.tensor(y).long()          # Convert to long tensor for labels\n",
    "\n",
    "# Create DataLoader\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Evaluate the model\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # No need to compute gradients during testing\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Get the predicted class\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            \n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            \n",
    "            # Save predictions and labels for additional evaluation if needed\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(target.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(test_loader)\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0685, Test Accuracy: 0.9801\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()  # For multi-class classification\n",
    "\n",
    "epoch_loss, epoch_acc, all_preds, all_labels = test(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {epoch_loss:.4f}, Test Accuracy: {epoch_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
