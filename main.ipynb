{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task A: Deep Learning for ECG Heartbeat Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we evaluate all trained models on unseen data from 'mitbih_test.csv'. We first do a comparison of basic models and then compare the performance of hybrid models trained on original and augmented datasets to see if data augmentation can help our models to achieve better classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Get current CUDA device index (if available)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current CUDA device index:\", torch.cuda.current_device())\n",
    "    print(\"CUDA device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    print(\"No CUDA devices found.\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from common_utils import get_dataloader, set_and_get_seed\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "SEED = set_and_get_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"Heartbeat_Dataset/mitbih_test.csv\", header=None)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of basic models\n",
    "\n",
    "In this section, we evaluate 3 basic models, namely GRU, CNN and Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    0.0: \"N\",\n",
    "    1.0: \"S\",\n",
    "    2.0: \"V\",\n",
    "    3.0: \"F\",\n",
    "    4.0: \"Q\"\n",
    "}\n",
    "\n",
    "test_df.iloc[:, -1] = test_df.iloc[:, -1].replace(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = test_df.iloc[:, :-1]\n",
    "y_label = test_df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y_label) \n",
    "\n",
    "X = x_data\n",
    "X_test = np.expand_dims(X, axis=1)  \n",
    "print(X_test.shape) \n",
    "X_test_tensor = torch.tensor(X_test).float()    \n",
    "y_test_tensor = torch.tensor(y).long()        \n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils import CNN1D\n",
    "\n",
    "# Load model\n",
    "model_path = \"./model/cnn_model.pth\"\n",
    "num_classes = len(label_encoder.classes_)\n",
    "loaded_CNNmodel = CNN1D(num_classes).to(DEVICE)  \n",
    "loaded_CNNmodel.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils import evaluateCNN_model\n",
    "\n",
    "# Evaluate\n",
    "test_accuracy = evaluateCNN_model(x_data, y_label, num_classes, loaded_CNNmodel, batch_size=32, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading transformer modules and constants\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "batch_size = 128\n",
    "num_workers = 0\n",
    "\n",
    "input_size = 200\n",
    "num_classes = 5\n",
    "num_heads = 5\n",
    "depth = 6\n",
    "max_epochs = 22\n",
    "lr = 1e-4\n",
    "dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./heartbeat_Dataset\"\n",
    "from transformer_eval import LitTransformer, LitMITBIH\n",
    "Transformer_model = LitTransformer(input_size, num_classes, num_heads, depth, max_epochs, lr, dropout)\n",
    "datamodule = LitMITBIH(path, batch_size, num_workers, length=input_size)\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./model/\"\n",
    "ckpt_name = \"ecg-transformer\"\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    dirpath=os.path.join(save_path, \"checkpoints\"),\n",
    "    filename=ckpt_name,\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor='val_acc',\n",
    "    # monitor='test_acc',\n",
    "    mode='max',\n",
    ")\n",
    "\n",
    "trainer = Trainer(accelerator=\"gpu\",\n",
    "                  devices=1,\n",
    "                  max_epochs=max_epochs,\n",
    "                  logger=False,\n",
    "                  callbacks=[model_checkpoint]\n",
    "                )\n",
    "\n",
    "print(f\"Loading checkpoint: {ckpt_name}.ckpt\")\n",
    "Transformer_model = Transformer_model.load_from_checkpoint(\n",
    "    os.path.join(save_path, \"checkpoints\", ckpt_name+\".ckpt\")\n",
    ")\n",
    "\n",
    "trainer.test(Transformer_model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils import GRUModel\n",
    "\n",
    "model_path = \"./model/gru.pth\"\n",
    "inputSize = 1\n",
    "hiddenSize = 64\n",
    "numClasses = 5\n",
    "numLayers = 1\n",
    "\n",
    "# Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Load model\n",
    "rnn_model = GRUModel(inputSize, hiddenSize, numLayers, numClasses).to(DEVICE)\n",
    "rnn_model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "reverse_labels = {v: k for k, v in labels.items()}\n",
    "\n",
    "# Test dataset for RNN\n",
    "RNN_X_test = test_df.iloc[:, :-1].values  \n",
    "RNN_y_test = test_df.iloc[:, -1].replace(reverse_labels).astype(float).values  # Convert labels back to numbers\n",
    "\n",
    "RNN_X_test = np.reshape(RNN_X_test, (RNN_X_test.shape[0], RNN_X_test.shape[1], 1))\n",
    "print(RNN_X_test.shape)\n",
    "\n",
    "RNN_test_loader = get_dataloader(RNN_X_test, RNN_y_test, False)\n",
    "\n",
    "test_loss = 0.0\n",
    "correct = 0.0\n",
    "y_pred, y_true = [], []\n",
    "\n",
    "# Get test results\n",
    "with torch.no_grad():  # No need to calculate gradients for validation\n",
    "    for X_batch, y_batch in RNN_test_loader:\n",
    "        X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "        outputs = rnn_model(X_batch)\n",
    "        loss = criterion(outputs, y_batch.long())\n",
    "        test_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        # Get predictions\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        correct += (predictions == y_batch).sum().item()\n",
    "\n",
    "        y_pred.append(predictions.float().detach().cpu())\n",
    "        y_true.append(y_batch.cpu())\n",
    "\n",
    "# Calculate average validation loss and accuracy\n",
    "test_loss /= len(RNN_test_loader.dataset)\n",
    "test_acc = correct / len(RNN_test_loader.dataset)\n",
    "\n",
    "preds = torch.cat(y_pred).numpy()\n",
    "truths = torch.cat(y_true).numpy()\n",
    "\n",
    "# Get classification report and confusion matrix\n",
    "classes = ['N', 'S', 'V', 'F', 'Q']\n",
    "labels = [0.0, 1.0, 2.0, 3.0, 4.0]\n",
    "report = pd.DataFrame(classification_report(truths, preds, labels=labels, target_names=classes, output_dict=True)).transpose()\n",
    "cm = confusion_matrix(truths, preds)\n",
    "\n",
    "# Print test loss and accuracy\n",
    "print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the results obtained from the different models\n",
    "| Model        | Test accuracy |\n",
    "|--------------|---------------|\n",
    "| CNN          | 0.9823        |\n",
    "| Transformers | 0.9779        |\n",
    "| RNN          | 0.8279        |\n",
    "| RNN(LSTM)    | 0.8278        |\n",
    "| RNN(GRU)     | 0.9740        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid models\n",
    "\n",
    "Based on the performance of individual models, we decided to explore hybrid architectures to leverage the strengths of different models. Consequently, we developed Transformer-CNN and CNN-GRU hybrid models. In the following section, we will evaluate the performance of the hybrid models which have been trained on the original and augmented datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils import cnn_transformer_evaluate\n",
    "from CNN_Transformer_hybrid import CNNTransformerHybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of model trained on original train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model/cnn_transformer_model.pth\"\n",
    "\n",
    "CNN_transformer_model = CNNTransformerHybrid(\n",
    "    input_dim=187, \n",
    "    num_classes=5,  \n",
    "    num_heads=8, \n",
    "    num_layers=6  \n",
    ").to(DEVICE)\n",
    "\n",
    "CNN_transformer_model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "\n",
    "epoch_loss, epoch_acc, all_preds, all_labels = cnn_transformer_evaluate(CNN_transformer_model, test_loader, criterion, DEVICE)\n",
    "print(f\"Loading model from: {model_path}\")\n",
    "print(f\"Test Loss: {epoch_loss:.4f} | Test Accuracy: {epoch_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of model trained on signal transformation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model/cnn_transformer_model_augment.pth\"\n",
    "\n",
    "CNN_transformer_model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "cnn_transformer_augment_epoch_loss, cnn_transformer_augment_epoch_acc, cnn_transformer_augment_all_preds, cnn_transformer_augment_all_labels = cnn_transformer_evaluate(CNN_transformer_model, test_loader, criterion, DEVICE)\n",
    "print(f\"Loading model from: {model_path}\")\n",
    "print(f\"Test Loss: {cnn_transformer_augment_epoch_loss:.4f} | Test Accuracy: {cnn_transformer_augment_epoch_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of model trained on SMOTE train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model/cnn_transformer_model_smote.pth\"\n",
    "\n",
    "CNN_transformer_model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "cnn_transformer_smote_epoch_loss, cnn_transformer_smote_epoch_acc, cnn_transformer_smote_all_preds, cnn_transformer_smote_all_labels = cnn_transformer_evaluate(CNN_transformer_model, test_loader, criterion, DEVICE)\n",
    "print(f\"Loading model from: {model_path}\")\n",
    "print(f\"Test Loss: {cnn_transformer_smote_epoch_loss:.4f} | Test Accuracy: {cnn_transformer_smote_epoch_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations about the best performing CNN-Transformers model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(cnn_transformer_augment_all_labels, cnn_transformer_augment_all_preds, labels=[1, 3, 4, 0, 2])\n",
    "\n",
    "# Plot confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['N', 'S', 'V', 'F', 'Q'])\n",
    "disp.plot( values_format=\"d\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds, truths = test(CNN_transformer_model, test_loader, device)\n",
    "# report = classification_report(truths, preds, labels=[0.0, 1.0, 2.0, 3.0, 4.0], target_names=['N', 'S', 'V', 'S', 'Q'], output_dict=True)\n",
    "# conf_matrix = confusion_matrix(truths, preds, labels=[0.0, 1.0, 2.0, 3.0, 4.0])\n",
    "\n",
    "report = classification_report(cnn_transformer_augment_all_labels, cnn_transformer_augment_all_preds, labels=[1, 3, 4, 0, 2], target_names=['N', 'S', 'V', 'F', 'Q'], output_dict=True)\n",
    "\n",
    "# Convert the report dictionary to a DataFrame and display\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model           | Dataset                | Test Loss | Test Accuracy |\n",
    "| -------------   | ---------------------- | --------- | ------------- |\n",
    "| CNN-Transformer | Original               | 0.0666    | 98.50%        |\n",
    "|                 | SMOTE                  | 0.0920    | 98.28%        |\n",
    "|                 | Signal Transformation  | 0.0671    | 98.55%        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-GRU model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of model trained on original train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device=DEVICE):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    y_pred, y_true = [], []\n",
    "\n",
    "    with torch.no_grad():  # No need to calculate gradients for validation/testing\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch.long())\n",
    "            test_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            correct += (predictions == y_batch).sum().item()\n",
    "\n",
    "            y_pred.append(predictions.float().detach().cpu())\n",
    "            y_true.append(y_batch.cpu())\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = correct / len(test_loader.dataset)\n",
    "    preds = torch.cat(y_pred).numpy()\n",
    "    truths = torch.cat(y_true).numpy()\n",
    "\n",
    "    return preds, truths, test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_data = pd.read_csv('Heartbeat_Dataset/mitbih_test.csv', header=None)\n",
    "X_test = test_data.iloc[:, :-1].values\n",
    "y_test = test_data.iloc[:, -1].values\n",
    "\n",
    "X_test = np.expand_dims(X_test, axis=1)  # Change axis from 2 to 1\n",
    "\n",
    "test_loader = get_dataloader(X_test, y_test, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils import CNN_GRU\n",
    "\n",
    "# Load model\n",
    "cnn_gru_model = CNN_GRU()\n",
    "model_path = './model/cnn_gru_original.pth'\n",
    "cnn_gru_model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "\n",
    "# Test model\n",
    "truths, preds, test_loss, test_acc = test(cnn_gru_model, test_loader)\n",
    "\n",
    "# Get classification report and confusion matrix\n",
    "classes_list = ['N', 'S', 'V', 'F', 'Q']\n",
    "labels_list = [0.0, 1.0, 2.0, 3.0, 4.0]\n",
    "report = pd.DataFrame(classification_report(truths, preds, labels=labels_list, target_names=classes_list, output_dict=True)).transpose()\n",
    "cm = confusion_matrix(truths, preds)\n",
    "\n",
    "# Print test loss and accuracy\n",
    "print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display report\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of model trained on SMOTE train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils import CNN_GRU\n",
    "\n",
    "# Load model\n",
    "cnn_gru_model = CNN_GRU()\n",
    "model_path = './model/cnn_gru_smote.pth'\n",
    "cnn_gru_model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "\n",
    "# Test model\n",
    "truths, preds, test_loss, test_acc = test(cnn_gru_model, test_loader)\n",
    "\n",
    "# Get classification report and confusion matrix\n",
    "report = pd.DataFrame(classification_report(truths, preds, labels=labels_list, target_names=classes_list, output_dict=True)).transpose()\n",
    "cm = confusion_matrix(truths, preds)\n",
    "\n",
    "# Print test loss and accuracy\n",
    "print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_acc* 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display report\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of model trained on Signal Transformation train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils import CNN_GRU\n",
    "\n",
    "# Load model\n",
    "cnn_gru_model = CNN_GRU()\n",
    "model_path = './model/cnn_gru_st.pth'\n",
    "cnn_gru_model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "\n",
    "# Test model\n",
    "truths, preds, test_loss, test_acc = test(cnn_gru_model, test_loader)\n",
    "\n",
    "# Get classification report and confusion matrix\n",
    "report = pd.DataFrame(classification_report(truths, preds, labels=labels_list, target_names=classes_list, output_dict=True)).transpose()\n",
    "cm = confusion_matrix(truths, preds)\n",
    "\n",
    "# Print test loss and accuracy\n",
    "print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display report\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model           | Dataset                | Test Loss | Test Accuracy |\n",
    "| -------------   | ---------------------- | --------- | ------------- |\n",
    "| CNN-Transformer | Original               | 0.0666    | 98.50%        |\n",
    "|                 | SMOTE                  | 0.0920    | 98.28%        |\n",
    "|                 | Signal Transformation  | 0.0671    | 98.55%        |\n",
    "| CNN-GRU         | Original               | 0.0718    | 98.70%        |\n",
    "|                 | SMOTE                  | 0.1385    | 98.68%        |\n",
    "|                 | Signal Transformation  | 0.0808    | 98.78%        |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
